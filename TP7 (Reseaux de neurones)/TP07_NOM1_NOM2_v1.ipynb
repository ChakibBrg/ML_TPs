{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID-TP07. Réseaux de neurones\n",
    "\n",
    "Dans ce TP, nous allons voir les réseaux de neurones.\n",
    "Premierement, nous allons implémenter la rétro-propagation, une fonction d'activation et une fonction du cout.\n",
    "Ensuite, nous allons tester l'effet de l'initialisation des paramètres, les fonctions d'activation, ainsi que les fonctions d'optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binômes : \n",
    "- **Binôme 1 :** Tirichine Mohammed\n",
    "- **Binôme 2 :** Bourzag Mohamed Chakib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Voici un exemple d'un reseau de neurones :\n",
    "\n",
    "![exemple](RNPA-exp.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Afin de mettre à jour** $w_{11}^{(4)}$\n",
    "$$\\frac{\\partial J}{\\partial w_{11}^{(4)}} = \\overbrace{\\frac{\\partial J}{\\partial f_{1}^{(4)}} \\frac{\\partial f_{1}^{(4)}}{\\partial z_{1}^{(4)}}}^{\\delta_{1}^{(4)}} \\overbrace{\\frac{\\partial z_{1}^{(4)}}{\\partial w_{11}^{(4)}}}^{a_{1}^{(3)}}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial f_{1}^{(4)}} = \\frac{(0.840, 0.843) - (0, 1)}{(0.840, 0.843) - (0.840, 0.843)^2} \n",
    "= (6.25, -1.186)$$\n",
    "\n",
    "$$\\frac{\\partial f_{1}^{(4)}}{\\partial z_{1}^{(4)}} = (0.840, 0.843) (0.160, 0.157) = (0.134, 0.132)$$\n",
    "\n",
    "$$\\delta_{1}^{(4)} = (6.25, -1.186) (0.134, 0.132) \\approx (0.838, -0.157)$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{11}^{(4)}} = moy((0.838, -0.157) (0.555, 0.612)) \n",
    "\\approx moy(0.465, -0.096) = 0.184$$\n",
    "\n",
    "---\n",
    "\n",
    "**Afin de mettre à jour** $w_{21}^{(4)}$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{21}^{(4)}} = \\overbrace{\\frac{\\partial J}{\\partial f_{1}^{(4)}} \\frac{\\partial f_{1}^{(4)}}{\\partial z_{1}^{(4)}}}^{\\delta_{1}^{(4)}} \\overbrace{\\frac{\\partial z_{1}^{(4)}}{\\partial w_{21}^{(4)}}}^{a_{2}^{(3)}}$$\n",
    "\n",
    "$$\\delta_{1}^{(4)} = (0.838, -0.157)$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{21}^{(4)}} = moy((0.838, -0.157) (0.386, 0.360)) \n",
    "\\approx moy(0.323, -0.056) = 0.134$$\n",
    "\n",
    "---\n",
    "\n",
    "**Afin de mettre à jour** $w_{11}^{(3)}$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{11}^{(3)}} = \n",
    "\\overbrace{\n",
    "\t\\overbrace{\n",
    "\t\t\\frac{\\partial J}{\\partial f_{1}^{(4)}} \n",
    "\t\t\\frac{\\partial f_{1}^{(4)}}{\\partial z_{1}^{(4)}}\n",
    "\t}^{\\delta_{1}^{(4)}} \n",
    "\t\\overbrace{\n",
    "\t\t\\frac{\\partial z_{1}^{(4)}}{\\partial f_{1}^{(3)}}\n",
    "\t}^{w_{11}^{(4)}} \n",
    "\t\\frac{\\partial f_{1}^{(3)}}{\\partial z_{1}^{(3)}} \n",
    "}^{\\delta_{1}^{(3)}} \n",
    "\\overbrace{\n",
    "\t\\frac{\\partial z_{1}^{(3)}}{\\partial w_{11}^{(3)}}\n",
    "}^{a_{1}^{(2)}}\n",
    "\\text{ Ici, on utilise l'ancien } w_{11}^{(4)}$$\n",
    "\n",
    "$$\\frac{\\partial f_{1}^{(3)}}{\\partial z_{1}^{(3)}} \\approx \n",
    "(0.555, 0.612) (0.445, 0.388) = (0.247, 0.237)$$\n",
    "\n",
    "$$\\delta_{1}^{(3)} = (0.838, -0.157) * 0.7 * (0.247, 0.237) \\approx (0.145, -0.026)$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{11}^{(2)}} = moy((0.145, -0.026) (0.622, 0.900)) \n",
    "= moy(0.090, -0.023) = 0.033$$\n",
    "\n",
    "---\n",
    "\n",
    "**Afin de mettre à jour** $w_{12}^{(3)}$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{12}^{(3)}} = \n",
    "\\overbrace{\n",
    "\t\\overbrace{\n",
    "\t\t\\frac{\\partial J}{\\partial f_{1}^{(4)}} \n",
    "\t\t\\frac{\\partial f_{1}^{(4)}}{\\partial z_{1}^{(4)}}\n",
    "\t}^{\\delta_{1}^{(4)}} \n",
    "\t\\overbrace{\n",
    "\t\t\\frac{\\partial z_{1}^{(4)}}{\\partial f_{2}^{(3)}}\n",
    "\t}^{w_{11}^{(4)}} \n",
    "\t\\frac{\\partial f_{2}^{(3)}}{\\partial z_{2}^{(3)}} \n",
    "}^{\\delta_{2}^{(3)}} \n",
    "\\overbrace{\n",
    "\t\\frac{\\partial z_{2}^{(3)}}{\\partial w_{12}^{(3)}}\n",
    "}^{a_{1}^{(2)}}\n",
    "\\text{ Ici, on utilise l'ancien } w_{12}^{(4)}$$\n",
    "\n",
    "$$\\frac{\\partial f_{2}^{(3)}}{\\partial z_{2}^{(3)}} \\approx \n",
    "(0.386, 0.360) (0.614, 0.64) = (0.237, 0.230)$$\n",
    "\n",
    "$$\\delta_{2}^{(3)} = (0.838, -0.157) * 0.7 * (0.237, 0.230) \\approx (0.139, -0.025)$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{11}^{(2)}} = moy((0.139, -0.025) (0.622, 0.900)) \n",
    "= moy(0.086, -0.023) = 0.032$$\n",
    "\n",
    "---\n",
    "\n",
    "**Afin de mettre à jour** $w_{11}^{(2)}$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_{11}^{(2)}} = \n",
    "\\overbrace{ \n",
    "\\left(\n",
    "\t\\delta_{1}^{(3)} \n",
    "\t\\overbrace{\n",
    "\t\t\\frac{\\partial z_{1}^{(3)}}{\\partial f_{1}^{(2)}}\n",
    "\t}^{w_{11}^{(3)}}\n",
    "\t+\n",
    "\t\\delta_{2}^{(3)} \n",
    "\t\\overbrace{\n",
    "\t\t\\frac{\\partial z_{2}^{(3)}}{\\partial f_{1}^{(2)}}\n",
    "\t}^{w_{12}^{(3)}}\n",
    "\\right)\n",
    "\\frac{\\partial f_{1}^{(2)}}{\\partial z_{1}^{(2)}}\n",
    "}^{\\delta_{1}^{(2)}} \n",
    "\\overbrace{\n",
    "\t\\frac{\\partial z_{1}^{(2)}}{\\partial w_{11}^{(2)}}\n",
    "}^{a_{1}^{(1)}}$$\n",
    "\n",
    "$$\\frac{\\partial f_{1}^{(2)}}{\\partial z_{1}^{(2)}} = (0.622, 0.900) (0.378, 0.100) = (0.235, 0.09)$$\n",
    "\n",
    "$$\\delta_{1}^{(2)} = \\left((0.145, -0.026) * (0.3) + (0.139, -0.025) * (-0.1)\\right) * (0.235, 0.09) \\approx (0.006956, -0.00047683)$$\n",
    "\n",
    "---\n",
    "\n",
    "**Cas general**\n",
    "\n",
    "On calcule les $\\delta^{(l)}$ où $l$ est le numéro de la couche\n",
    "\n",
    "$$\\delta^{(sortie)} = \n",
    "\\frac{\\partial J}{\\partial f^{(sortie)}} \\frac{\\partial f^{(sortie)}}{\\partial z^{(sortie)}}\n",
    "\\,,\\,\n",
    "\\delta^{(l)} = \\frac{\\partial f^{(l)}}{\\partial z^{(l)}} w^{(l+1)} \\delta^{(l+1)}$$\n",
    "\n",
    "On calcule les gradients\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w^{(l)}} = a^{(l-1)} \\delta^{(l)}\n",
    "\\,,\\,\n",
    "\\frac{\\partial J}{\\partial b^{(l)}} = \\delta^{(l)}$$\n",
    "\n",
    "On met à jour les paramètres\n",
    "\n",
    "$$w = w - \\alpha \\frac{\\partial J}{\\partial w^{(l)}}\n",
    "\\,,\\,\n",
    "b = b - \\alpha \\frac{\\partial J}{\\partial b^{(l)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.26.1', '2.1.2', '3.8.1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib      import colors \n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing          import Tuple, List, Type, Union\n",
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Ici, nous définissons un API (une sorte d'interfaces) pour les fonctions d'activation et les fonctions du cout. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API (Ne modifier pas ; c'est juste une interface/classe abstraite)\n",
    "class Activation(object): \n",
    "    # Calculer l'activation en se basant sur Z (la somme linéaire)\n",
    "    def activer(self, Z):\n",
    "        pass\n",
    "    # Calculer la dérivée en se basant sur Z et l'activation A\n",
    "    def deriver(self, Z, H):\n",
    "        pass\n",
    "\n",
    "# API (Ne modifier pas ; c'est juste une interface/classe abstraite)\n",
    "class Cout(object): \n",
    "    # Calculer l'activation en se basant sur Z (la somme linéaire)\n",
    "    def calculer(self, H, Y):\n",
    "        pass\n",
    "    # Calculer la dérivée en se basant sur Z et l'activation A\n",
    "    def deriver(self, H, Y):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Fonctions d'activation\n",
    "\n",
    "L'activation logistique est calculée comme :\n",
    "$$A = \\sigma(Z) = \\frac{1}{1+e^{-Z}}$$\n",
    "\n",
    "La dérivée partielle est donnée par :\n",
    "$$\\frac{\\partial \\sigma(Z)}{\\partial \\theta} = \\sigma(Z) (1-\\sigma(Z))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.84104179, 0.84290453]), array([0.1336905 , 0.13241648]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Dérivée de la fonction d'activation logistique\n",
    "def d_sigmaf(Z, A): \n",
    "    return A * (1 - A)\n",
    "\n",
    "# Rien à programmer ici\n",
    "def sigmaf(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "class Logistique(Activation):\n",
    "    def activer(self, Z):\n",
    "        return sigmaf(Z)\n",
    "    def deriver(self, Z, H):\n",
    "        return d_sigmaf(Z, H)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array([0.84104179, 0.84290453]), array([0.1336905 , 0.13241648]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "logistique = Logistique()\n",
    "z4_1       = np.array([1.666, 1.68])\n",
    "a4_1       = logistique.activer(z4_1)\n",
    "a4_1p      = logistique.deriver(z4_1, a4_1)\n",
    "\n",
    "a4_1, a4_1p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Fonctions du coût\n",
    "\n",
    "La fonction BCE est calaculée par :\n",
    "$$BCE = - ( Y \\log(H) + (1-Y) \\log(1-H))$$\n",
    "\n",
    "Sa dérivée est calculée par :\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta} = \\frac{H-Y}{H - H^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.83258146, 0.17078832]), array([ 6.25      , -1.18623962]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Dérivée de la fonction d'erreur BCE\n",
    "def d_bcef(H, Y):\n",
    "    return (H - Y) / (H - H**2)\n",
    "\n",
    "def bcef(H, Y):\n",
    "    return - (Y * np.log(H) + (1-Y) * np.log(1-H))\n",
    "\n",
    "class BCE(Cout):\n",
    "    def calculer(self, H, Y):\n",
    "        return bcef(H, Y)\n",
    "    def deriver(self, H, Y):\n",
    "        return d_bcef(H, Y)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array([1.83258146, 0.17078832]), array([ 6.25      , -1.18623962]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "bce = BCE()\n",
    "\n",
    "H = np.array([0.840 , 0.843])\n",
    "Y = np.array([0., 1.])\n",
    "J = bce.calculer(H, Y)\n",
    "DJ = bce.deriver(H, Y)\n",
    "\n",
    "J, DJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Neurone\n",
    "\n",
    "$$\\delta^{(l)} = \\frac{\\partial f^{(l)}}{\\partial z^{(l)}} w^{(l+1)} \\delta^{(l+1)}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w^{(l)}} = a^{(l-1)} \\delta^{(l)}$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b^{(l)}} = \\delta^{(l)}$$\n",
    "\n",
    "Le produit est un produit matriciel (sur $M$) et il faut prendre la moyenne des sorties (sur $Ln$). \n",
    "\n",
    "La fonction qui met à jour les paramètres prend en entrée : \n",
    "- $W[Lp]$ une liste des poids; un vecteur de taille $Lp$ (le nombre des neurones de la couche précédente)\n",
    "- $b$ le biais \n",
    "- $Z[M]$ la combinaison linéaire du neurone courant; un vecteur de taille $M$ (le nombre des échantillons)\n",
    "- $A[M]$ l'activation du neurone courant; un vecteur de taille $M$  \n",
    "- $A\\_past[M, Lp]$ les activations des neurones de la couche précédente; une matrice de taille est $(M * Lp)$\n",
    "- $Delta\\_next[M, Ln]$ le delta calculé dans la couche suivante; une matrice de taille $M * Ln$ ($Ln$ : le nombre des neurones dans la couche suivante)\n",
    "- $W\\_next[Ln]$ les poids vers la couche suivante; un vecteur de taille $Ln$\n",
    "- $act$ c'est un object de type \"Activation\"; il fournit deux méthodes : \"act.activer\" et \"act.deriver\"\n",
    "- $alpha$ le pas de l'entraînement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.48750437, 0.2093472 ]),\n",
       " -0.30324311474187016,\n",
       " array([ 0.00696306, -0.00047683]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Rétro-propagation (neurone)\n",
    "def neurone_maj(W, b, Z, A, A_past, Delta_next, W_next, act, alpha=1.):\n",
    "    Delta = act.deriver(Z, A) * (W_next @ Delta_next.T)\n",
    "    bn    = b - alpha * np.mean(Delta)\n",
    "    Wn    = W - alpha * (A_past.T @ Delta)\n",
    "    return Wn, bn, Delta\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array([0.49375218, 0.2046736 ]),\n",
    "#  -0.30324311474187016,\n",
    "#  array([ 0.00696306, -0.00047683]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "W_t = np.array([0.5, 0.2])\n",
    "b_t = -0.3\n",
    "Z_t = np.array([0.5, 2.2])\n",
    "# M (l'activation actuelle)\n",
    "A_t = np.array([0.62245933, 0.90024951])\n",
    "# M * L (les activations de la couche précédente)\n",
    "A_past_t = np.array([[2., -1.], [3., 5.]])\n",
    "# L\n",
    "Delta_next_t = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "W_next_t = np.array([0.3, -0.1])\n",
    "act = Logistique() #la fonction d'activation\n",
    "\n",
    "W_nouv, b_nouv, Delta_nouv = neurone_maj(W_t, b_t, Z_t, A_t, A_past_t, Delta_next_t, W_next_t, act, alpha=1.)\n",
    "\n",
    "W_nouv, b_nouv, Delta_nouv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z2_1 = [0.5 2.2]\n",
      "a2_1 = [0.62245933 0.90024951]\n",
      "derivee(a2_1) = [0.23500371 0.08980033]\n",
      "ancien b = -0.3\n",
      "ancien w = [0.5 0.2]\n",
      "delta2 = [ 0.00696306 -0.00047683]\n",
      "nouveaux b = -0.30324311473938026\n",
      "nouveaux w = [0.48750437 0.2093472 ]\n"
     ]
    }
   ],
   "source": [
    "class Neurone(object):\n",
    "    def __init__(self, taille_entree, activation=Logistique()):\n",
    "        self.b   = 0.\n",
    "        self.w   = np.array([0.] * taille_entree)\n",
    "        self.act = activation\n",
    "        \n",
    "    def randomiser(self):\n",
    "        self.w = np.random.rand(len(self.w))\n",
    "        self.b = np.random.rand(1)[0]\n",
    "        \n",
    "    def __aggreger(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def activer(self, X):\n",
    "        self.a_past = X\n",
    "        self.z      = self.__aggreger(X)\n",
    "        self.a      = self.act.activer(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def actualiser(self, delta_next, w_next, alpha=1.):\n",
    "        w_ancien              = self.w.copy()\n",
    "        self.w, self.b, delta = neurone_maj(self.w, self.b, self.z, self.a, self.a_past, \n",
    "                                            delta_next, w_next, self.act, alpha=alpha)\n",
    "        return delta, w_ancien\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# z2_1 = [0.5 2.2]\n",
    "# a2_1 = [0.62245933 0.90024951]\n",
    "# derivee(a2_1) = [0.23500371 0.08980033]\n",
    "# ancien b = -0.3\n",
    "# ancien w = [0.5 0.2]\n",
    "# delta2 = [ 0.00696306 -0.00047683]\n",
    "# nouveaux b = -0.30324311473938026\n",
    "# nouveaux w = [0.49375218 0.2046736 ]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# Céation d'un neurone avec deux entrées\n",
    "n = Neurone(2)\n",
    "# ---------------------\n",
    "#On ne doit pas affecter les poids directement \n",
    "#Ici, c'est juste pour avoir les mêmes poids du neurone de sortie dans l'exemple du cours\n",
    "# On va reproduire les paramètres du neurone 1 couche cachée 1 (couche 2)\n",
    "n.b = -0.3\n",
    "n.w = np.array([0.5, 0.2])\n",
    "# ---------------------\n",
    "\n",
    "# M X Lp (ici c'est X : couche d'entrée)\n",
    "A1 = np.array([[2., -1.], [3., 5.]])\n",
    "# M X Ln (Delta de la couche suivante)\n",
    "Delta3 = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "W3_1 = np.array([0.3, -0.1])\n",
    "\n",
    "\n",
    "A2_1 = n.activer(A1)\n",
    "print(\"z2_1 = \" + str(n.z))\n",
    "print(\"a2_1 = \" + str(A2_1))\n",
    "# la dérivée de la fonction logistique n'a pas besoin de z, donc on passe 0\n",
    "print(\"derivee(a2_1) = \" + str(n.act.deriver(0,A2_1)))\n",
    "print(\"ancien b = \" + str(n.b))\n",
    "\n",
    "Delta2, W2_ancien = n.actualiser(Delta3, W3_1) \n",
    "\n",
    "print(\"ancien w = \" + str(W2_ancien))\n",
    "print(\"delta2 = \" + str(Delta2))\n",
    "print(\"nouveaux b = \" + str(n.b))\n",
    "print(\"nouveaux w = \" + str(n.w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Couche\n",
    "\n",
    "**Rien à programmer ici.**\n",
    "\n",
    "Une classe qui définit une couche en indiquant le nombre des neurones (taille), le nombre de ces entrées et la fonction d'activation de ces neurones.\n",
    "Cette classe comprend 3 méthodes : \n",
    "- randomiser : initialiser les paramètres des neurones d'une façon aléatoire\n",
    "- propagation_avant : appliquer la propagatation avant \n",
    "- retro_propagation : appliquer la rétropropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activations : [[0.62245933 0.66818777]\n",
      " [0.90024951 0.96770454]]\n",
      "deltas : [[ 0.00696306  0.00682726]\n",
      " [-0.00047683 -0.00017109]]\n"
     ]
    }
   ],
   "source": [
    "class Couche(object):\n",
    "    \n",
    "    def __init__(self, taille, taille_entree, activation=logistique):\n",
    "        self.neurones = [Neurone(taille_entree, activation=activation) for i in range(taille)]\n",
    "        \n",
    "    def randomiser(self):\n",
    "        for neurone in self.neurones:\n",
    "            neurone.randomiser()\n",
    "\n",
    "    def propagation_avant(self, X):\n",
    "        activations = []\n",
    "        for neurone in self.neurones:\n",
    "            activations.append(neurone.activer(X))\n",
    "        return np.array(activations).T\n",
    "    \n",
    "    def retro_propagation(self, delta_next, W_next, alpha=1.):\n",
    "        W_anciens = []\n",
    "        Deltas = []\n",
    "        for i, neurone in enumerate(self.neurones):\n",
    "            delta, w_ancien = neurone.actualiser(delta_next, W_next[i], alpha=alpha)\n",
    "            W_anciens.append(w_ancien)\n",
    "            Deltas.append(delta)\n",
    "        return np.array(Deltas).T, np.array(W_anciens).T\n",
    "\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# activations : [[0.62245933 0.66818777]\n",
    "#  [0.90024951 0.96770454]]\n",
    "# deltas : [[ 0.00696306  0.00682726]\n",
    "#  [-0.00047683 -0.00017109]]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "# la couche 2 \n",
    "c2 = Couche(2, 2)\n",
    "\n",
    "#On ne doit pas affecter les poids directement \n",
    "#Ici, c'est juste pour avoir les mêmes poids du neurone de sortie dans l'exemple du cours\n",
    "c2.neurones[0].b = -0.3\n",
    "c2.neurones[0].w = np.array([0.5, 0.2])\n",
    "c2.neurones[1].b = 0.5\n",
    "c2.neurones[1].w = np.array([0.3, 0.4])\n",
    "\n",
    "a2 = np.array([[2., -1.], [3., 5.]])\n",
    "# L\n",
    "delta3 = np.array([[ 0.14523862, -0.02613822], [ 0.1394202, -0.02531591]]).T\n",
    "w3 = np.array([[0.3, -0.1],[0.5, -0.3]])\n",
    "\n",
    "# M X Lp (ici c'est X : couche d'entrée)\n",
    "a1 = np.array([[2., -1.], [3., 5.]])\n",
    "a2 = c2.propagation_avant(a1)\n",
    "print(\"activations : \" + str(a2))\n",
    "\n",
    "Deltas2, W_anciens2 = c2.retro_propagation(delta3, w3)\n",
    "\n",
    "print(\"deltas : \" + str(Deltas2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Réseau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le cout = 1.0020916974430965\n",
      "w4_1 = [0.32989252 0.43184158]\n",
      "w3_1 = [0.23312579 0.42824741]\n",
      "w3_2 = [-0.16399277 -0.36866055]\n",
      "w2_1 = [0.48750437 0.2093472 ]\n",
      "w2_2 = [0.28685874 0.40768269]\n",
      "la prédiction : [0 1]\n"
     ]
    }
   ],
   "source": [
    "class RN(object):\n",
    "    def __init__(self, taille_entree, cout=bce, alpha=1.):\n",
    "        self.taille_courante = taille_entree #la taille de la dernière couche\n",
    "        self.cout = cout #objet de type Cout pour calculer le cout et sa dérivée\n",
    "        self.alpha = alpha\n",
    "        self.couches = []\n",
    "\n",
    "    def ajouter_couche(self, taille, activation=logistique):\n",
    "        nouv_couche = Couche(taille, self.taille_courante, activation=activation)\n",
    "        self.couches.append(nouv_couche)\n",
    "        self.taille_courante = taille\n",
    "        \n",
    "    def randomiser(self):\n",
    "        for couche in self.couches:\n",
    "            couche.randomiser()\n",
    "    \n",
    "    def predire(self, X): \n",
    "        Y = X\n",
    "        if self.norm:\n",
    "            Y = np.where(self.std==0, X, (X - self.mean)/self.std)\n",
    "            \n",
    "        for couche in self.couches:\n",
    "            Y = couche.propagation_avant(Y)\n",
    "        if Y.ndim == 2 and Y.shape[1] == 1:\n",
    "            Y = Y.flatten()\n",
    "        return np.where(Y < 0.5, 0, 1)\n",
    "    \n",
    "    \n",
    "    def _faire_iteration(self, X, Y):\n",
    "        # propagation avant\n",
    "        a = X\n",
    "        for couche in self.couches:\n",
    "            a = couche.propagation_avant(a)\n",
    "            \n",
    "        # calcul du cout et sa dérivée \n",
    "        YY = np.array(Y)\n",
    "        if YY.ndim < 2 : \n",
    "            YY = YY[:, np.newaxis]\n",
    "        J = np.mean(self.cout.calculer(a, YY))\n",
    "        J_prime = self.cout.deriver(a, YY)\n",
    "        \n",
    "        # retropropagation \n",
    "        w_past = np.array([[1.] * self.taille_courante])\n",
    "        delta_past = J_prime\n",
    "        for couche in reversed(self.couches): # on commance de la dernière couche vers la première\n",
    "            delta_past, w_past = couche.retro_propagation(delta_past, w_past)\n",
    "        return J\n",
    "    \n",
    "    def entrainer(self, X, Y, nbr_it=100, norm=False):\n",
    "        couts = []\n",
    "        X_norm = X\n",
    "        self.norm = norm\n",
    "        if norm:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            self.std = np.std(X, axis=0)\n",
    "            X_norm = np.where(self.std==0, X, (X - self.mean)/self.std)\n",
    "\n",
    "        for i in range(nbr_it): \n",
    "            J = self._faire_iteration(X_norm, Y)\n",
    "            couts.append(J)\n",
    "        return couts\n",
    "    \n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# le cout = 1.0020916974430965\n",
    "# w4_1 = [0.51494626 0.56592079]\n",
    "# w3_1 = [0.2665629 0.4641237]\n",
    "# w3_2 = [-0.13199638 -0.33433028]\n",
    "# w2_1 = [0.49375219 0.2046736 ]\n",
    "# w2_2 = [0.29342937 0.40384135]\n",
    "# la prédiction : [0 1]\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X = np.array([[2., -1.], [3., 5.]])\n",
    "Y = np.array([0., 1.])\n",
    "\n",
    "rn = RN(2) #deux caractéristiques d'entrée\n",
    "rn.ajouter_couche(2) #ajouter une couche avec 2 neurones (cachée)\n",
    "rn.ajouter_couche(2) #ajouter une couche avec 2 neurones (cachée)\n",
    "rn.ajouter_couche(1) #ajouter une couche avec 1 neurone (sortie)\n",
    "\n",
    "#On ne doit pas affecter les poids directement \n",
    "#Ici, c'est juste pour avoir les mêmes poids du neurone de sortie dans l'exemple du cours\n",
    "rn.couches[0].neurones[0].b = -0.3\n",
    "rn.couches[0].neurones[0].w = np.array([0.5, 0.2])\n",
    "rn.couches[0].neurones[1].b = 0.5\n",
    "rn.couches[0].neurones[1].w = np.array([0.3, 0.4])\n",
    "\n",
    "rn.couches[1].neurones[0].b = -0.3\n",
    "rn.couches[1].neurones[0].w = np.array([0.3, 0.5])\n",
    "rn.couches[1].neurones[1].b = -0.2\n",
    "rn.couches[1].neurones[1].w = np.array([-0.1, -0.3])\n",
    "\n",
    "rn.couches[2].neurones[0].b = 1.\n",
    "rn.couches[2].neurones[0].w = np.array([0.7, 0.7])\n",
    "\n",
    "J = rn._faire_iteration(X, Y)\n",
    "\n",
    "print(\"le cout = \" + str(J))\n",
    "print(\"w4_1 = \" + str(rn.couches[2].neurones[0].w))\n",
    "print(\"w3_1 = \" + str(rn.couches[1].neurones[0].w))\n",
    "print(\"w3_2 = \" + str(rn.couches[1].neurones[1].w))\n",
    "print(\"w2_1 = \" + str(rn.couches[0].neurones[0].w))\n",
    "print(\"w2_2 = \" + str(rn.couches[0].neurones[1].w))\n",
    "\n",
    "rn.entrainer(X, Y, nbr_it=200)\n",
    "print(\"la prédiction : \" + str(rn.predire(X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diabetes2\n",
    "diabetes   = pd.read_csv(\"data/diabetes2.csv\") \n",
    "X_diabetes = diabetes.iloc[:, :-1].values  \n",
    "Y_diabetes = diabetes.iloc[:,  -1].values\n",
    "\n",
    "# Cette configuration est mise en place comme ceci exprès\n",
    "# C'est pour tester le cas où la régression est difavorisée\n",
    "NBR_TEST   = 240\n",
    "# Supposant que les 30% premières lignes sont pour le test et le reste pour l'entraînement\n",
    "X_test     = X_diabetes[-NBR_TEST:, :] # 30% ou plus\n",
    "Y_test     = Y_diabetes[-NBR_TEST:   ].reshape([-1, 1])\n",
    "\n",
    "X_train    = X_diabetes[:-NBR_TEST, :] \n",
    "Y_train    = Y_diabetes[:-NBR_TEST   ].reshape([-1, 1])\n",
    "\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler   = StandardScaler()\n",
    "X_trains = scaler.fit_transform(X_train)\n",
    "X_tests  = scaler.transform(X_test)\n",
    "\n",
    "X_trains[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1. Paramètres initiaux et complexité\n",
    "\n",
    "Nous voulons tester l'intêt de l'initialisation des paramètres (thétas) et la complexité du modèle.\n",
    "Pour ce faire, cinq modèles ont été entrainés afin de récupérer l'erreur d'entrainement et de la validation. \n",
    "Les modèles testé sont :\n",
    "- **Log0** : Un seul neurone (régression logistique) avec initialisation 0\n",
    "- **LogR** : Un seul neurone (régression logistique) avec initialisation aléatoire\n",
    "- **RN0** : Un réseau de neurones 4(relu)X2(relu)X1(sigmoid) avec initialisation 0\n",
    "- **RN1** : Un réseau de neurones 4(relu)X2(relu)X1(sigmoid) avec initialisation 1\n",
    "- **RNR** : Un réseau de neurones 4(relu)X2(relu)X1(sigmoid) avec initialisation aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow              import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "alpha  = 0.01\n",
    "NBR_IT = 200\n",
    "\n",
    "M, N = X_train.shape\n",
    "\n",
    "# ==================================\n",
    "# Définition des modèles\n",
    "# ==================================\n",
    "\n",
    "modeles = {}\n",
    "\n",
    "modeles['Log0'] = Sequential()\n",
    "modeles['Log0'].add(Dense(1, activation=\"sigmoid\", kernel_initializer='zero', bias_initializer='zeros'))\n",
    "\n",
    "modeles['LogR'] = Sequential()\n",
    "modeles['LogR'].add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "\n",
    "modeles['RN0']  = Sequential()\n",
    "modeles['RN0'].add(Dense(4, activation=\"relu\", kernel_initializer='zero', bias_initializer='zeros'))\n",
    "modeles['RN0'].add(Dense(2, activation=\"relu\", kernel_initializer='zero', bias_initializer='zeros'))\n",
    "modeles['RN0'].add(Dense(1, activation=\"sigmoid\", kernel_initializer='zero', bias_initializer='zeros'))\n",
    "\n",
    "modeles['RN1']  = Sequential()\n",
    "modeles['RN1'].add(Dense(4, activation=\"relu\", kernel_initializer='one', bias_initializer='one'))\n",
    "modeles['RN1'].add(Dense(2, activation=\"relu\", kernel_initializer='one', bias_initializer='one'))\n",
    "modeles['RN1'].add(Dense(1, activation=\"sigmoid\", kernel_initializer='one', bias_initializer='one'))\n",
    "\n",
    "modeles['RNR']  = Sequential()\n",
    "modeles['RNR'].add(Dense(4, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "modeles['RNR'].add(Dense(2, activation=\"relu\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "modeles['RNR'].add(Dense(1, activation=\"sigmoid\", kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform'))\n",
    "\n",
    "# ==================================\n",
    "# Entrainement des modèles\n",
    "# ==================================\n",
    "\n",
    "# on n'affiche pas les 3 premières itérations, le temps que le modèle se stabilise\n",
    "# sinon, un modèle peut avoir une grande valeur par rapport aux autres \n",
    "# donc, on ne peut pas visualiser la convergence des autres\n",
    "IT_range = range(NBR_IT)[3:]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,5))\n",
    "\n",
    "\n",
    "for nom, modele in modeles.items():\n",
    "    modele.compile(loss      = tf.keras.losses.binary_crossentropy,\n",
    "                 optimizer = tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "    print(nom, ': Entrainement ...')\n",
    "    results = modele.fit(X_trains, Y_train, epochs=NBR_IT, validation_data=(X_tests, Y_test), verbose=0)\n",
    "    \n",
    "    # ===========================\n",
    "    # PREPARATION DE L'AFFICHAGE\n",
    "    # ===========================\n",
    "    ax1.plot(IT_range, results.history[\"loss\"    ][3:], label=nom)\n",
    "    ax2.plot(IT_range, results.history[\"val_loss\"][3:], label=nom)\n",
    "\n",
    "# ==================================\n",
    "# Affichage \n",
    "# ==================================\n",
    "\n",
    "ax1.title.set_text(\"Entrainement\")\n",
    "ax2.title.set_text(\"Validation\")\n",
    "\n",
    "ax1.set(xlabel='iteration', ylabel='erreur')\n",
    "ax2.set(xlabel='iteration', ylabel='erreur')\n",
    "\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les résultats**\n",
    "- Nous remarquons que les modèles avec un seul neurone sont plus rapides que les modèles de réseau de neurones (en terme des itérations et en terme de temps). Pourquoi ?\n",
    "- Nous remarquons que **RN0** ne s'améliore pas (il stagne dès les premières itérations). Expliquer pourquoi.\n",
    "- Nous remarquons que **RN1** s'améliore par rapport à **RN0**, mais il stagne rapidement par rapport **RNR**. Expliquer pourquoi.\n",
    "- En se basant sur la validation, quelle est le rapport entre le nombre des couches, la complexité du problème, le nombre/qualité des données et les problèmes d'apprentissage (sous/sur). Mentionner toutes les combinaisons qui peuvent causer des problèmes (il a dit 4 combinaisons).\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2. Fonctions d'activation\n",
    "\n",
    "Nous voulons tester quelles sont les fonctions d'activation plus adéquates aux couches cachées et celles à la couche de sortie.\n",
    "Pour ce faire, cinq modèles ont été entrainés afin de récupérer l'historique de l'erreur d'entrainement. \n",
    "Les modèles testé sont :\n",
    "- **relu->sigm** : un réseau avec **relu** dans les couches cachées et **sigmoid** dans la couche de sortie\n",
    "- **sigm->sigm** : un réseau avec **sigmoid** dans les couches cachées et **sigmoid** dans la couche de sortie\n",
    "- **tanh->sigm** : un réseau avec **tanh** dans les couches cachées et **sigmoid** dans la couche de sortie\n",
    "- **sigm->relu** : un réseau avec **sigmoid** dans les couches cachées et **relu** dans la couche de sortie\n",
    "- **relu->relu** : un réseau avec **relu** dans les couches cachées et **relu** dans la couche de sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha  = 0.01\n",
    "NBR_IT = 100\n",
    "\n",
    "M, N = X_train.shape\n",
    "\n",
    "L1 = 2 # Nombre des neurones dans la couche 1\n",
    "L2 = 2 # Nombre des neurones dans la couche 2\n",
    "\n",
    "# ==================================\n",
    "# Définition des modèles\n",
    "# ==================================\n",
    "\n",
    "defs = [ # Les définitions\n",
    "    ('relu->sigm', 'relu', 'sigmoid'),\n",
    "    ('sigm->sigm', 'sigmoid', 'sigmoid'),\n",
    "    ('tanh->sigm', 'tanh', 'sigmoid'),\n",
    "    ('sigm->relu', 'sigmoid', 'relu'),\n",
    "    ('relu->relu', 'relu', 'relu')\n",
    "]\n",
    "\n",
    "modeles = {}\n",
    "params = {'kernel_initializer':'glorot_uniform', 'bias_initializer':'glorot_uniform'}\n",
    "\n",
    "for nom, in_act, out_act in defs:\n",
    "    modeles[nom] = Sequential()\n",
    "    modeles[nom].add(Dense(L1, activation = in_act , **params))\n",
    "    modeles[nom].add(Dense(L2, activation = in_act , **params))\n",
    "    modeles[nom].add(Dense(1,  activation = out_act, **params))\n",
    "\n",
    "# ==================================\n",
    "# Entrainement des modèles\n",
    "# ==================================\n",
    "\n",
    "results = {}\n",
    "\n",
    "for nom, modele in modeles.items():\n",
    "    modele.compile(loss      = tf.keras.losses.binary_crossentropy,\n",
    "                 optimizer = tf.keras.optimizers.SGD(learning_rate=alpha))\n",
    "    print(nom, ': Entrainement ...')\n",
    "    results[nom] = modele.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "    \n",
    "\n",
    "# ==================================\n",
    "# Affichage \n",
    "# ==================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,5))\n",
    "\n",
    "for nom, result in results.items():\n",
    "    ax = ax1 if nom.endswith('sigm') else ax2\n",
    "    ax.plot(range(NBR_IT), result.history['loss'], label=nom)\n",
    "    \n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"erreur\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyser les résultats**\n",
    "- Nous remarquons que le modèle **sigmoid->sigmoid** a stagné rapidement. Expliquer comment ?\n",
    "- Nous remarquons que ce modèle a convergé plus rapidement (en terme de nombre des itérations) par rapport aux deux modèles avec sortie **sigmoid**. Pourquoi ?\n",
    "- Nous remarquons que les modèles avec sortie **relu** ne sont pas stables ; à chaque exécution, nous aurons un diagramme différent (des fois amélioration, des fois détérioration, etc.). Il faut noter que l'initialisation aléatoire n'est pas la source du problème vu qu'il y a d'autres modèles similaires mais stables. Donc, pourquoi nous avons eu ce comportement ?\n",
    "\n",
    "**Réponse**\n",
    "- ...\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.3. Fonctions d'optimisation\n",
    "\n",
    "Nous voulons tester des différentes fonctions d'optimisation.\n",
    "Pour ce faire, quatres modèles ont été entrainés afin de récupérer l'historique de l'erreur d'entrainement. \n",
    "Les modèles testés sont :\n",
    "- **GD** : un réseau entrainé avec la descente des gradients\n",
    "- **Adagrad** : un réseau entrainé avec AdaGrad\n",
    "- **RMSprop** : un réseau entrainé avec RMSprop\n",
    "- **Adam** : un réseau entrainé avec Adam\n",
    "\n",
    "**Chercher sur Internet les formules de chacune de ces fonctions d'optimisation afin de pouvoir repondre aux questions suivantes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha  = 0.01\n",
    "NBR_IT = 300\n",
    "\n",
    "M, N = X_train.shape\n",
    "\n",
    "L1 = 2\n",
    "L2 = 2\n",
    "\n",
    "# on n'affiche pas les 3 premières itérations, le temps que le modèle se stabilise\n",
    "# sinon, un modèle peut avoir une grande valeur par rapport aux autres \n",
    "# donc, on ne peut pas visualiser la convergence des autres\n",
    "IT_range = range(NBR_IT)[3:]\n",
    "\n",
    "defs = [ # Les définitions\n",
    "    ('GD'     , tf.keras.optimizers.SGD    (learning_rate=alpha)),\n",
    "    ('Adagrad', tf.keras.optimizers.Adagrad(learning_rate=alpha)),\n",
    "    ('RMSprop', tf.keras.optimizers.RMSprop(learning_rate=alpha)),\n",
    "    ('Adam'   , tf.keras.optimizers.Adam   (learning_rate=alpha))\n",
    "]\n",
    "\n",
    "params = {'kernel_initializer':'glorot_uniform', 'bias_initializer':'glorot_uniform'}\n",
    "\n",
    "for nom, opt in defs:\n",
    "    modele = Sequential()\n",
    "    modele.add(Dense(L1, activation=\"relu\"   , **params))\n",
    "    modele.add(Dense(L2, activation=\"relu\"   , **params))\n",
    "    modele.add(Dense(1,  activation=\"sigmoid\", **params))\n",
    "    modele.compile(loss      = tf.keras.losses.binary_crossentropy,\n",
    "                   optimizer = opt)\n",
    "    \n",
    "    print(nom, ': entrainement ...')\n",
    "    results = modele.fit(X_trains, Y_train, epochs=NBR_IT, verbose=0)\n",
    "    plt.plot(IT_range, results.history[\"loss\"][3:], label=nom)\n",
    "\n",
    "\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"erreur\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser les résultats**\n",
    "- Nous remarquons que le modèle **GD** converge plus vite que **AdaGrad**. Pourquoi ?\n",
    "- Pourquoi **RMSprop** converge plus vite que **AdaGrad**, pourtant leurs equations sont presque similaire ? (ici, vous devez expliquer l'apport dans l'equation du premier par rapport au deuxième)\n",
    "- En exécutant ce code plusieurs fois, nous remarquons que Adam est plus stable. Pourquoi ?\n",
    "\n",
    "**Réponse**\n",
    "- ... (Le alpha diminue à chaque fois dans AdaGrad ?)\n",
    "- ... \n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
